# -*- coding: utf-8 -*-
"""CAP 4770 - Project 2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GTxDkE1y-gn45G1XQykD7dPSn5eOd2Tf

#Project 2 - Diabetes Prediction

Source: https://www.cdc.gov/brfss/annual_data/annual_2022.html

Used: https://1drv.ms/f/s!AlOnTDugth0xjd9JdpknjeS_r-h05Q?e=IkU5fw
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold, GridSearchCV

# reading in the file
df = pd.read_csv('BRFSS22.csv')
print(df.head())
print(df.info())

# selecting the target attributes
target = ['_STATE', 'SEXVAR', '_HLTHPLN', 'MEDCOST1', '_TOTINDA', '_MICHD', 'CVDSTRK3', '_LTASTH1', 'CHCCOPD3',
          'ADDEPEV3', 'CHCKDNY2', 'VETERAN3', 'DIFFWALK', 'SMOKE100', '_DRDXAR2', '_METSTAT', 'DRNKANY6', 'DISPCODE',
          'PHYSHLTH', 'MENTHLTH', 'SLEPTIM1', '_BMI5', '_AGEG5YR', '_RACE1', '_EDUCAG', 'EMPLOY1', '_INCOMG1',
          '_BMI5CAT', 'GENHLTH', 'MARITAL', 'DIABETE4']
df = df[target]
print(df.head())
print(df.shape)

# unique_states = np.unique(df['_STATE'])
# print(unique_states)

unique_dia = np.unique(df['DIABETE4'])
print(unique_dia)

# Normalization
df['DIABETE4'] = df['DIABETE4'].apply(lambda x: 1 if x in [1, 2, 4] else 0)  # 1 2 4 diabetic (4 prediabetic)
df['EMPLOY1'] = df['EMPLOY1'].apply(lambda x: 1 if x in [1, 2, 6] else 0)  # 1 2 and 6 employed (6 is student)
df['_EDUCAG'] = df['_EDUCAG'].apply(lambda x: 1 if x in [3, 4] else 0)  # 3 and 4 higher education
df['_BMI5CAT'] = df['_BMI5CAT'].apply(lambda x: 1 if x in [1, 2] else 0)  # 1 under weight and
df['MARITAL'] = df['MARITAL'].apply(lambda x: 1 if x in [1, 6] else 0)  # 1 6 married and couple

df['SEXVAR'] = df['SEXVAR'].apply(lambda x: 1 if x in [1] else 0)
df['_HLTHPLN'] = df['_HLTHPLN'].apply(lambda x: 1 if x in [1] else 0)
df['MEDCOST1'] = df['MEDCOST1'].apply(lambda x: 1 if x in [1] else 0)
df['_TOTINDA'] = df['_TOTINDA'].apply(lambda x: 1 if x in [1] else 0)
df['_MICHD'] = df['_MICHD'].apply(lambda x: 1 if x in [2] else 0)
df['CVDSTRK3'] = df['CVDSTRK3'].apply(lambda x: 1 if x in [2] else 0)
df['_LTASTH1'] = df['_LTASTH1'].apply(lambda x: 1 if x in [1] else 0)
df['CHCCOPD3'] = df['CHCCOPD3'].apply(lambda x: 1 if x in [2] else 0)
df['ADDEPEV3'] = df['ADDEPEV3'].apply(lambda x: 1 if x in [2] else 0)
df['CHCKDNY2'] = df['CHCKDNY2'].apply(lambda x: 1 if x in [2] else 0)
df['VETERAN3'] = df['VETERAN3'].apply(lambda x: 1 if x in [2] else 0)
df['DIFFWALK'] = df['DIFFWALK'].apply(lambda x: 1 if x in [2] else 0)
df['SMOKE100'] = df['SMOKE100'].apply(lambda x: 1 if x in [2] else 0)
df['_DRDXAR2'] = df['_DRDXAR2'].apply(lambda x: 1 if x in [2] else 0)
df['_METSTAT'] = df['_METSTAT'].apply(lambda x: 1 if x in [1] else 0)
df['DRNKANY6'] = df['DRNKANY6'].apply(lambda x: 1 if x in [2] else 0)
df['DISPCODE'] = df['DISPCODE'].apply(lambda x: 1 if x in [1] else 0)

df['PHYSHLTH'] = df['PHYSHLTH'].apply(lambda x: 1 if x in [88] else 0)
df['MENTHLTH'] = df['MENTHLTH'].apply(lambda x: 1 if x in [88] else 0)
unique_dia = np.unique(df['DIABETE4'])

# Removing instances with missing values
missing_rows = df[df.apply(lambda row: row.astype(str).str.contains('nan').any(), axis=1)]
print(f"Number of rows with missing values: {len(missing_rows)}")
print("DF length before removal:", len(df))
df = df[~df.index.isin(missing_rows.index)]
print("DF length after removal:", len(df))

# Min-Max Normalization
columns_to_scale = ['PHYSHLTH', 'MENTHLTH', 'SLEPTIM1', '_BMI5', '_AGEG5YR', '_RACE1', '_EDUCAG', 'EMPLOY1', '_INCOMG1',
                    '_STATE', '_BMI5CAT', 'GENHLTH']
scaler = MinMaxScaler()
df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])
print(df.head())

# df.to_csv('testfile.csv')

# creating test train split
census_data = df.drop('DIABETE4', axis=1)
labels = df['DIABETE4']
print(labels.unique())

X_train, X_test, y_train, y_test = train_test_split(census_data, labels, test_size=.90, random_state=42)
print("X train shaape", X_train.shape)

print("Y train head", y_train.head())

# setting up the model
# mlp = MLPClassifier(max_iter=500, early_stopping=True)ASL grid
# mlp = MLPClassifier(early_stopping=True, solver='adam') LMAL grid
mlp = MLPClassifier(early_stopping=True, solver='adam', learning_rate_init=.01, activation='relu', learning_rate='constant', hidden_layer_sizes=23)

cross_validation = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
cv_scores= cross_val_score(mlp,X_train, y_train, cv=cross_validation, scoring='accuracy')
print(f"Mean accuracy: {cv_scores.mean():.4f}")
mlp.fit(X_train, y_train)
y_pred = mlp.predict(X_test)
print(classification_report(y_test, y_pred))


# parameter grid for grid search
# param_grid = { ASL grid (Activation, Solver, Learning Rate)
#     'activation': ['relu', 'tanh', 'identity', 'logistic'],
#     'solver': ['lbfgs', 'sgd', 'adam'],
#     'learning_rate': ['constant', 'invscaling', 'adaptive']
# } Winner; solver=adam
# param_grid = {#LMAL(learning rate#, max iteration, activation, and learning rate) grid
#     'learning_rate_init': [.000001, .00001, .0001, .001, .01, .1],
#     'max_iter': [100, 250, 500, 750, 1000, 1250, 1500, 2000, 2500, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000],
#     'activation': ['relu', 'tanh', 'identity', 'logistic'],
#     'learning_rate': ['constant', 'invscaling', 'adaptive']
# }
# param_grid = {
    #'hidden_layer_sizes': [(23), (23,23), (23,23,23),(23,23,23,23,), (23,23,23,23,23,)],
    #'max_iter': [100, 500, 1000, 2000, 4000, 6000, 8000]
    #'max_iter': [10,20,30,40,50,60,70,80,90,100]
    #'max_iter': [10,20,30,40,50,60,70,80,90,100,500, 1000, 2000, 4000, 6000, 8000,10000,20000,40000,60000,80000,100000]
# }

# Preforming test
# grid_search = GridSearchCV(mlp, param_grid, cv=cross_validation, scoring='accuracy', verbose=1, n_jobs=-1)
# grid_search.fit(X_train, y_train)
#
# # Getting results
# print("Best Parameters:", grid_search.best_params_)
# best_mlp = grid_search.best_estimator_
# test_accuracy = best_mlp.score(X_test, y_test)
# print(f"Test Accuracy: {test_accuracy:.4f}")
#
# results = grid_search.cv_results_
# params = results['params']
# mean_test_scores = results['mean_test_score']
#
#
# y_pred = best_mlp.predict(X_test)
#
# print("Best results: ", classification_report(y_test, y_pred, digits=4))
#
# # Making a confusion matrix for visualization
confusion_df = pd.DataFrame(confusion_mtx, index=['Not Diabetic', 'Diabetic'], columns=['Not Diabetic', 'Diabetic'])
sns.heatmap(cm, annot=True, fmt='g', cmap='Greens', xticklabels=["Not Diabetic", "Diabetic"],
            yticklabels=["Not Diabetic", "Diabetic"])
plt.title("Best results")
plt.show()
#
# # Prep for a scatter/bar graph
# param_combinations = [str(params) for params in results['params']]
#
# # Combine the parameter combinations and mean test scores into a list of tuples
# results_list = list(zip(param_combinations, mean_test_scores))
#
# # Sort the results by mean test score in descending order
# sorted_results = sorted(results_list, key=lambda x: x[1], reverse=True)
#
# # Extract the top 5 and bottom 5 results
# top_5 = sorted_results[:5]
# bottom_5 = sorted_results[-5:]
#
# # Unpack the tuples for plotting
# top_params, top_scores = zip(*top_5)
# bottom_params, bottom_scores = zip(*bottom_5)
#
# # Plotting the scatter plot
# fig, ax = plt.subplots(figsize=(20, 12))
#
# # Plot the top 5 results in green as scatter points
# ax.scatter(top_params, [score * 100 for score in top_scores], color='green', label='Top 5', marker='x')
# # Plot the bottom 5 results in red as scatter points
# ax.scatter(bottom_params, [score * 100 for score in bottom_scores], color='red', label='Bottom 5', marker='x')
# # Customize the plot
# ax.set_ylabel('Accuracy Score (%)')
# ax.set_title('Results from Grid search')
# ax.set_xticks(range(len(top_params) + len(bottom_params)))
# ax.set_xticklabels(top_params + bottom_params, rotation=45, ha='right')
# ax.legend()
#
# # Show the plot
# plt.tight_layout()
# plt.show()
# #print(results)
#
# # Print mean_test_score and corresponding params for hidden layer and epochs vs accuracy.
# for mean_score, param in zip(mean_test_scores, params):
#     print(f"Mean Test Score: {mean_score:.4f}, Parameters: {param}")
